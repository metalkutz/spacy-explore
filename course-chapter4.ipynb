{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Training a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Training and updating models \n",
    "\n",
    "Training entity recognizer\n",
    "- The entity recognizer tags words and phrases in context\n",
    "- Each token can only be part of one entity\n",
    "- Examples need to come with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"iPhone X is coming\")\n",
    "doc.ents = [Span(doc, 0, 2, label=\"GADGET\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texts with no entities are also important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I need a new phone! Any tips?\")\n",
    "doc.ents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: teach the model to generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training and evaluation data\n",
    "\n",
    "Generating a training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create a Doc with entity spans\n",
    "doc1 = nlp(\"iPhone X is coming\")\n",
    "doc1.ents = [Span(doc1, 0, 2, label=\"GADGET\")]\n",
    "# Create another doc without entity spans\n",
    "doc2 = nlp(\"I need a new phone! Any tips?\")\n",
    "\n",
    "docs = [doc1, doc2]  # and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spliting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.shuffle(docs)\n",
    "train_docs = docs[:len(docs) // 2]\n",
    "dev_docs = docs[len(docs) // 2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DocBin: container to efficiently store and save Doc objects\n",
    "- can be saved to a binary file\n",
    "- binary files are used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training import DocBin\n",
    "\n",
    "# Create and save a collection of training docs\n",
    "train_docbin = DocBin(docs=train_docs)\n",
    "train_docbin.to_disk(\"./train.spacy\")\n",
    "# Create and save a collection of evaluation docs\n",
    "dev_docbin = DocBin(docs=dev_docs)\n",
    "dev_docbin.to_disk(\"./dev.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCyâ€™s rule-based Matcher is a great way to quickly create training data for named entity models. A list of sentences is available as the variable TEXTS. You can print it to inspect it. We want to find all mentions of different iPhone models, so we can create training data to teach a model to recognize them as \"GADGET\".\n",
    "\n",
    "- Write a pattern for two tokens whose lowercase forms match \"iphone\" and \"x\".\n",
    "- Write a pattern for two tokens: one token whose lowercase form matches \"iphone\" and a digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone 8]\n",
      "[iPhone 11, iPhone 8]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "with open(\"iphone.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match \"iphone\" and \"x\"\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Token whose lowercase form matches \"iphone\" and a digit\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add patterns to the matcher and create docs with matched entities\n",
    "matcher.add(\"GADGET\", [pattern1, pattern2])\n",
    "docs = []\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=match_id) for match_id, start, end in matches]\n",
    "    print(spans)\n",
    "    doc.ents = spans\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the data for our corpus, we need to save it out to a .spacy file. The code from the previous example is already available.\n",
    "\n",
    "- Instantiate the DocBin with the list of docs.\n",
    "- Save the DocBin to a file called train.spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "doc_bin = DocBin(docs=docs)\n",
    "doc_bin.to_disk(\"train.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Configuring and running the training\n",
    "\n",
    "The training config\n",
    "- single source of truth for all settings\n",
    "- typically called config.cfg\n",
    "- defines how to initialize the nlp object\n",
    "- includes all settings about the pipeline components and their model implementations\n",
    "- configures the training process and hyperparameters\n",
    "- makes your training more reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "toml"
    }
   },
   "outputs": [],
   "source": [
    "[nlp]\n",
    "lang = \"en\"\n",
    "pipeline = [\"tok2vec\", \"ner\"]\n",
    "batch_size = 1000\n",
    "\n",
    "[nlp.tokenizer]\n",
    "@tokenizers = \"spacy.Tokenizer.v1\"\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "\n",
    "[components.ner.model]\n",
    "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
    "hidden_width = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Generating a config\n",
    "\n",
    "- spaCy can auto-generate a default config file for you\n",
    "- interactive quickstart widget in the docs\n",
    "- init config command on the CLI\n",
    "\n",
    "```$ python -m spacy init config ./config.cfg --lang en --pipeline ner```\n",
    "- init config: the command to run\n",
    "- config.cfg: output path for the generated config\n",
    "- --lang: language class of the pipeline, e.g. en for English\n",
    "- --pipeline: comma-separated names of components to include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training a pipeline\n",
    "\n",
    "- all you need is the `config.cfg` and the training and development data\n",
    "- config settings can be overwritten on the command line\n",
    "\n",
    "```$ python -m spacy train ./config.cfg --output ./output --paths.train train.spacy --paths.dev dev.spacy```\n",
    "\n",
    "- train: the command to run\n",
    "- config.cfg: the path to the config file\n",
    "- --output: the path to the output directory to save the trained pipeline\n",
    "- --paths.train: override with path to the training data\n",
    "- --paths.dev: override with path to the evaluation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a trained pipeline\n",
    "\n",
    "- output after training is a regular loadable spaCy pipeline\n",
    "    - model-last: last trained pipeline\n",
    "    - model-best: best trained pipeline\n",
    "- load it with spacy.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"/path/to/output/model-best\")\n",
    "doc = nlp(\"iPhone 11 vs iPhone 8: What's the difference?\")\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packaging your pipeline\n",
    "\n",
    "- spacy package: create an installable Python package containing your pipeline\n",
    "- easy to version and deploy\n",
    "\n",
    "```$ python -m spacy package /path/to/output/model-best ./packages --name my_pipeline --version 1.0.0```\n",
    "\n",
    "```$ cd ./packages/en_my_pipeline-1.0.0```\n",
    "\n",
    "```$ pip install dist/en_my_pipeline-1.0.0.tar.gz```\n",
    "\n",
    "Load and use the pipeline after installation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Training multiple labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc1 = nlp(\"Reddit partners with Patreon to help creators build communities\")\n",
    "doc1.ents = [\n",
    "    Span(doc1, 0, 1, label=\"WEBSITE\"),\n",
    "    Span(doc1, 3, 4, label=\"WEBSITE\"),\n",
    "]\n",
    "\n",
    "doc2 = nlp(\"PewDiePie smashes YouTube record\")\n",
    "doc2.ents = [Span(doc2, 2, 3, label=\"WEBSITE\")]\n",
    "\n",
    "doc3 = nlp(\"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\")\n",
    "doc3.ents = [Span(doc3, 0, 1, label=\"WEBSITE\")]\n",
    "\n",
    "# And so on..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
